import sqlite3
import pandas as pd
import requests
from bs4 import BeautifulSoup
import os
import sys
import time

# ê²½ë¡œ ì„¤ì •
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
try:
    import db_setup as database
except ImportError:
    import database

class HankyungScraper:
    def __init__(self, db_path='expert_alpha_v3.db'):
        self.db_path = db_path

    def is_already_exists(self, title, date, expert_name):
        """DBë¥¼ ì¡°íšŒí•˜ì—¬ í•´ë‹¹ ë¦¬í¬íŠ¸ê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        query = "SELECT id FROM reports WHERE title = ? AND report_date = ? AND expert_name = ?"
        cur.execute(query, (title, date, expert_name))
        exists = cur.fetchone() is not None
        conn.close()
        return exists

    def fetch_data(self, pages=5):
        print(f"ğŸ“¡ í•œê²½ ì»¨ì„¼ì„œìŠ¤ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {pages}í˜ì´ì§€)...")
        new_count = 0
        
        # í•œê²½ ì»¨ì„¼ì„œìŠ¤ ê¸°ë³¸ URL (ì¢…ëª© ë¦¬í¬íŠ¸ ê¸°ì¤€)
        base_url = "http://consensus.hankyung.com/apps.analysis/analysis.list?skinType=stock"
        
        for page in range(1, pages + 1):
            url = f"{base_url}&pagenum=20&page={page}"
            # í•œê²½ì€ User-Agentê°€ ì—†ìœ¼ë©´ ê±°ë¶€í•  ìˆ˜ ìˆì–´ ì¶”ê°€í•©ë‹ˆë‹¤.
            headers = {'User-Agent': 'Mozilla/5.0'}
            res = requests.get(url, headers=headers)
            soup = BeautifulSoup(res.text, 'html.parser')
            
            table = soup.select_one('div.table_main table')
            if not table: break
            
            rows = table.select('tbody tr')
            duplicate_in_page = 0
            page_items = 0

            for row in rows:
                cols = row.select('td')
                if len(cols) < 6: continue
                
                page_items += 1
                date = cols[0].text.strip() # í•œê²½ì€ ë³´í†µ YYYY-MM-DD í˜•ì‹
                title = cols[2].text.strip()
                expert_name = cols[3].text.strip() # ì‘ì„±ì(ì• ë„ë¦¬ìŠ¤íŠ¸)
                
                # [ì§€ëŠ¥í˜• ì²´í¬] ì´ë¯¸ DBì— ìˆëŠ” ë¦¬í¬íŠ¸ì¸ê°€?
                if self.is_already_exists(title, date, expert_name):
                    duplicate_in_page += 1
                    continue
                
                # ì—¬ê¸°ì— ì†Œì¥ë‹˜ì˜ ê¸°ì¡´ ì €ì¥ ë¡œì§(INSERT ë¬¸ ë“±)ì´ ë“¤ì–´ê°‘ë‹ˆë‹¤.
                # ì˜ˆ: self.save_to_db(date, title, expert_name, ...)
                new_count += 1
            
            print(f"ğŸ“„ í•œê²½ {page}í˜ì´ì§€ ì™„ë£Œ: {page_items - duplicate_in_page}ê°œ ì‹ ê·œ ìˆ˜ì§‘")
            
            # [í•µì‹¬] í•´ë‹¹ í˜ì´ì§€ê°€ ì „ë¶€ ì¤‘ë³µì´ë©´ ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œë¡œ ê°„ì£¼í•˜ê³  ì¤‘ë‹¨
            if page_items > 0 and duplicate_in_page == page_items:
                print(f"ğŸ›‘ {page}í˜ì´ì§€ì—ì„œ ì¤‘ë³µ ë°ì´í„° ë°œê²¬. í•œê²½ ìˆ˜ì§‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
                
            time.sleep(0.5)

        print(f"âœ… í•œê²½ ì´ {new_count}ê°œì˜ ìƒˆë¡œìš´ ë¦¬í¬íŠ¸ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
