import sqlite3
import requests
from bs4 import BeautifulSoup
import time
import datetime

class HankyungScraper:
    def __init__(self, db_path='expert_alpha_v3.db'):
        self.db_path = db_path

    def is_already_exists(self, title, date, expert_name):
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        cur.execute("SELECT id FROM reports WHERE title = ? AND report_date = ? AND expert_name = ?", (title, date, expert_name))
        exists = cur.fetchone() is not None
        conn.close()
        return exists

    def fetch_data(self, pages=5):
        print(f"ğŸ“¡ í•œê²½ ì»¨ì„¼ì„œìŠ¤ ê°•ì œ ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {pages}í˜ì´ì§€)...")
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        
        # 1ë…„ ì „ë¶€í„° ì˜¤ëŠ˜ê¹Œì§€ë¡œ ë‚ ì§œ ë²”ìœ„ ê°•ì œ ì„¤ì •
        edate = datetime.datetime.now().strftime('%Y-%m-%d')
        sdate = (datetime.datetime.now() - datetime.timedelta(days=365)).strftime('%Y-%m-%d')
        
        new_count = 0
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        
        for page in range(1, pages + 1):
            # ê²€ìƒ‰ ì¡°ê±´(sdate, edate)ì„ URLì— ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€
            url = f"http://consensus.hankyung.com/apps.analysis/analysis.list?skinType=stock&sdate={sdate}&edate={edate}&pagenum=20&page={page}"
            
            try:
                res = requests.get(url, headers=headers, timeout=15)
                res.encoding = 'utf-8'
                soup = BeautifulSoup(res.text, 'html.parser')
                rows = soup.select('div.table_main table tbody tr')
                
                if not rows:
                    print(f"âš ï¸ {page}í˜ì´ì§€ì— í–‰(row)ì´ ì—†ìŠµë‹ˆë‹¤.")
                    break

                page_new_count = 0
                for row in rows:
                    cols = row.select('td')
                    if len(cols) < 5 or "ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤" in row.text: continue
                    
                    date = cols[0].text.strip()
                    title = cols[2].text.strip()
                    expert_name = cols[3].text.strip()
                    source = cols[4].text.strip()
                    
                    if self.is_already_exists(title, date, expert_name):
                        continue
                    
                    cur.execute('''
                        INSERT INTO reports (title, expert_name, source, report_date)
                        VALUES (?, ?, ?, ?)
                    ''', (title, expert_name, source, date))
                    page_new_count += 1
                    new_count += 1
                
                conn.commit()
                print(f"ğŸ“„ í•œê²½ {page}í˜ì´ì§€ ì™„ë£Œ: {page_new_count}ê°œ ì‹ ê·œ ì¶”ê°€")
                time.sleep(0.5)
                
            except Exception as e:
                print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
                break

        conn.close()
        print(f"âœ… í•œê²½ ì´ {new_count}ê°œ DB ì €ì¥ ì™„ë£Œ!")
