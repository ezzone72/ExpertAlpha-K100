import requests
from bs4 import BeautifulSoup
import sqlite3
import time

class HankyungScraper:
    def __init__(self, db_path='expert_alpha_v3.db'):
        self.db_path = db_path
        self.provider = "HANKYUNG"

    def fetch_data(self, pages=5):
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # í•œê²½ ì»¨ì„¼ì„œìŠ¤ ì¢…ëª©ë¶„ì„ í˜ì´ì§€
        base_url = "http://consensus.hankyung.com/apps.analysis/analysis.list?&sdate=2025-02-03&edate=2026-02-02&search_report_classify=RP_GW&page="
        
        print(f"ğŸ“¡ {self.provider} ìˆ˜ì§‘ ì‹œì‘...")

        for page in range(1, pages + 1):
            url = f"{base_url}{page}"
            resp = requests.get(url, headers=headers)
            # í•œê²½ì€ ì¸ì½”ë”© ì„¤ì •ì´ ì¤‘ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
            resp.encoding = 'euc-kr' 
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # ë¦¬ìŠ¤íŠ¸ í…Œì´ë¸” í–‰(tr) ì¶”ì¶œ
            rows = soup.select('div.table_style01 table tbody tr')
            
            for row in rows:
                cols = row.select('td')
                if len(cols) < 6: continue
                
                # 1. ë°ì´í„° ì¶”ì¶œ
                # í•œê²½ êµ¬ì¡°: [0]ë‚ ì§œ, [1]ì œëª©, [2]ì ì •ê°€, [3]íˆ¬ìì˜ê²¬, [4]ì‘ì„±ì, [5]ì œê³µì¶œì²˜
                date = cols[0].text.strip().replace('.', '-')
                title = cols[1].select_one('a').text.strip()
                # ì œëª©ì—ì„œ ì¢…ëª©ëª… ì¶”ì¶œ (ë³´í†µ "ì‚¼ì„±ì „ì(005930)" í˜•ì‹)
                raw_title = cols[1].text.strip()
                stock_name = raw_title.split('(')[0].strip() if '(' in raw_title else "Unknown_Stock"
                
                name = cols[4].text.strip() # ì‘ì„±ì ì‹¤ëª…
                org = cols[5].text.strip()  # ì¦ê¶Œì‚¬ëª…

                # 2. DB ì €ì¥ (sources)
                cur.execute("""
                    INSERT OR IGNORE INTO sources (name, type, provider, organization)
                    VALUES (?, ?, ?, ?)
                """, (name, 'ANALYST', self.provider, org))
                
                cur.execute("SELECT source_id FROM sources WHERE name = ? AND provider = ?", (name, self.provider))
                source_id = cur.fetchone()[0]
                
                # 3. DB ì €ì¥ (statements)
                cur.execute("""
                    INSERT INTO statements (source_id, stock_name, issue_date, title)
                    VALUES (?, ?, ?, ?)
                """, (source_id, stock_name, date, title))

            print(f"   - {self.provider} {page}í˜ì´ì§€ ì™„ë£Œ")
            time.sleep(0.5)

        conn.commit()
        conn.close()
        print(f"ğŸ {self.provider} ìˆ˜ì§‘ ì¢…ë£Œ.")

if __name__ == "__main__":
    scraper = HankyungScraper('../expert_alpha_v3.db')
    scraper.fetch_data(5)
