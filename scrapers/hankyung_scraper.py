import sqlite3
import requests
from bs4 import BeautifulSoup
import time
import os
import sys

class HankyungScraper:
    def __init__(self, db_path='expert_alpha_v3.db'):
        self.db_path = db_path

    def is_already_exists(self, title, date, expert_name):
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        # ì œëª©, ë‚ ì§œ, ì‘ì„±ìê°€ ëª¨ë‘ ê°™ìœ¼ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼
        cur.execute("SELECT id FROM reports WHERE title = ? AND report_date = ? AND expert_name = ?", (title, date, expert_name))
        exists = cur.fetchone() is not None
        conn.close()
        return exists

    def fetch_data(self, pages=5):
        print(f"ğŸ“¡ í•œê²½ ì»¨ì„¼ì„œìŠ¤ ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {pages}í˜ì´ì§€)...")
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        new_count = 0
        
        # ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ë³´ì´ê²Œ í•˜ëŠ” í•„ìˆ˜ í—¤ë”
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'http://consensus.hankyung.com/'
        }
        
        for page in range(1, pages + 1):
            # skinType=stock (ì¢…ëª©ë¦¬í¬íŠ¸) í˜ì´ì§• URL
            url = f"http://consensus.hankyung.com/apps.analysis/analysis.list?skinType=stock&pagenum=20&page={page}"
            
            try:
                res = requests.get(url, headers=headers, timeout=10)
                # í•œê²½ì€ EUC-KR ëŒ€ì‹  UTF-8ì„ ì“°ê¸°ë„ í•˜ì§€ë§Œ í•œê¸€ ê¹¨ì§ ë°©ì§€ë¥¼ ìœ„í•´ ì„¤ì •
                res.encoding = 'utf-8' 
                soup = BeautifulSoup(res.text, 'html.parser')
                
                # ë°ì´í„°ê°€ ë“¤ì–´ìˆëŠ” í…Œì´ë¸” í–‰(tr) ì°¾ê¸°
                rows = soup.select('div.table_main table tbody tr')
                
                if not rows or len(rows) <= 1:
                    print(f"âš ï¸ {page}í˜ì´ì§€ì—ì„œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (êµ¬ì¡° ë³€ê²½ ì˜ì‹¬)")
                    break

                duplicate_in_page = 0
                page_items = 0

                for row in rows:
                    cols = row.select('td')
                    # í•œê²½ ì¢…ëª©ë¦¬í¬íŠ¸ í…Œì´ë¸”ì€ ë³´í†µ 6~10ê°œì˜ tdë¥¼ ê°€ì§
                    if len(cols) < 5: continue
                    
                    page_items += 1
                    date = cols[0].text.strip() # ì‘ì„±ì¼ (YYYY-MM-DD)
                    title = cols[2].text.strip() # ì œëª©
                    expert_name = cols[3].text.strip() # ì‘ì„±ì
                    source = cols[4].text.strip() # ì¦ê¶Œì‚¬
                    
                    # ì¤‘ë³µ ì²´í¬
                    if self.is_already_exists(title, date, expert_name):
                        duplicate_in_page += 1
                        continue
                    
                    # DB ì €ì¥ (INSERT)
                    cur.execute('''
                        INSERT INTO reports (title, expert_name, source, report_date)
                        VALUES (?, ?, ?, ?)
                    ''', (title, expert_name, source, date))
                    new_count += 1
                
                conn.commit() # í˜ì´ì§€ ë‹¨ìœ„ ì»¤ë°‹
                print(f"ğŸ“„ í•œê²½ {page}í˜ì´ì§€ ì™„ë£Œ: {page_items - duplicate_in_page}ê°œ ì‹ ê·œ ìˆ˜ì§‘")
                
                # ëª¨ë“  ì•„ì´í…œì´ ì¤‘ë³µì´ë©´ ì¤‘ë‹¨
                if page_items > 0 and duplicate_in_page == page_items:
                    print(f"ğŸ›‘ ì¤‘ë³µ ë°ì´í„° ë°œê²¬. í•œê²½ ìˆ˜ì§‘ ì¢…ë£Œ.")
                    break
                    
                time.sleep(0.5) # ì°¨ë‹¨ ë°©ì§€ ì§€ì—°
                
            except Exception as e:
                print(f"âŒ í•œê²½ {page}í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬: {e}")
                break

        conn.close()
        print(f"âœ… í•œê²½ ì´ {new_count}ê°œ DB ì €ì¥ ì™„ë£Œ!")
