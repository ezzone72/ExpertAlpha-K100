import sqlite3
import requests
from bs4 import BeautifulSoup
import time
import datetime

class HankyungScraper:
    def __init__(self, db_path='expert_alpha_v3.db'):
        self.db_path = db_path

    def is_already_exists(self, title, date, expert_name):
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        cur.execute("SELECT id FROM reports WHERE title = ? AND report_date = ? AND expert_name = ?", (title, date, expert_name))
        exists = cur.fetchone() is not None
        conn.close()
        return exists

    def fetch_data(self, pages=5):
        print(f"ğŸ“¡ í•œê²½ ì»¨ì„¼ì„œìŠ¤ ëª¨ë°”ì¼ ê²½ë¡œ ì¹¨íˆ¬ ì‹œì‘ (ìµœëŒ€ {pages}í˜ì´ì§€)...")
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        
        # ë¸Œë¼ìš°ì € í—¤ë”ë¥¼ ëª¨ë°”ì¼ ê¸°ê¸°(ì•„ì´í°)ì²˜ëŸ¼ ìœ„ì¥
        headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Referer': 'http://m.consensus.hankyung.com/'
        }
        
        new_count = 0
        for page in range(1, pages + 1):
            # ğŸ’¡ ëª¨ë°”ì¼ìš© ë¦¬ìŠ¤íŠ¸ ì£¼ì†Œì…ë‹ˆë‹¤. (ì£¼ì†Œ êµ¬ì¡°ê°€ ë‹¤ë¦…ë‹ˆë‹¤)
            url = f"http://consensus.hankyung.com/apps.analysis/analysis.list?skinType=stock&pagenum=20&page={page}"
            
            try:
                res = requests.get(url, headers=headers, timeout=15)
                res.encoding = 'utf-8'
                soup = BeautifulSoup(res.text, 'html.parser')
                
                # ëª¨ë°”ì¼/PC í†µí•© êµ¬ì¡°ì—ì„œ tr ìš”ì†Œë¥¼ ë‹¤ ê°€ì ¸ì˜µë‹ˆë‹¤.
                rows = soup.select('tr')
                
                page_added = 0
                for row in rows:
                    cols = row.select('td')
                    if len(cols) < 5: continue
                    
                    # ë‚ ì§œ í˜•ì‹ ì²´í¬ (YYYY-MM-DD)
                    date_raw = cols[0].text.strip()
                    if len(date_raw) != 10 or "-" not in date_raw: continue
                    
                    title = cols[2].text.strip()
                    expert = cols[3].text.strip()
                    source = cols[4].text.strip()
                    
                    if self.is_already_exists(title, date_raw, expert):
                        continue
                    
                    cur.execute('''
                        INSERT INTO reports (title, expert_name, source, report_date)
                        VALUES (?, ?, ?, ?)
                    ''', (title, expert, source, date_raw))
                    page_added += 1
                    new_count += 1
                
                conn.commit()
                # ğŸ’¡ í•œê²½ë„ ì²« ì œëª©ì„ ì°ì–´ì„œ í˜ì´ì§€ê°€ ë„˜ì–´ê°€ëŠ”ì§€ í™•ì¸í•˜ê² ìŠµë‹ˆë‹¤.
                sample_title = rows[1].select('td')[2].text.strip()[:10] if len(rows) > 1 else "No Data"
                print(f"ğŸ“„ í•œê²½ {page}p ì™„ë£Œ: {page_added}ê°œ ì¶”ê°€ (ì²«ì œëª©: {sample_title}...)")
                
                if page_added == 0 and page > 10: # 10í˜ì´ì§€ ì—°ì† 0ê°œë©´ ì´ë¯¸ ë‹¤ ì±„ì›Œì§„ ê²ƒ
                    print("ğŸ í•œê²½ ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ êµ¬ê°„ ë„ë‹¬.")
                    break
                    
                time.sleep(0.7)
                
            except Exception as e:
                print(f"âŒ í•œê²½ {page}p ì—ëŸ¬: {e}")
                break

        conn.close()
        print(f"âœ… í•œê²½ ì´ {new_count}ê°œ DB ì €ì¥ ì™„ë£Œ!")
