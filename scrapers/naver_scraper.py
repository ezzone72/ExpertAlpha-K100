import sqlite3
import pandas as pd
import requests
from bs4 import BeautifulSoup
import os
import sys
import time

# ê²½ë¡œ ì„¤ì •
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
try:
    import db_setup as database
except ImportError:
    import database

class NaverScraper:
    def __init__(self, db_path='expert_alpha_v3.db'):
        self.db_path = db_path

    def is_already_exists(self, title, date, expert_name):
        """DBë¥¼ ì¡°íšŒí•˜ì—¬ í•´ë‹¹ ë¦¬í¬íŠ¸ê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        query = "SELECT id FROM reports WHERE title = ? AND report_date = ? AND expert_name = ?"
        cur.execute(query, (title, date, expert_name))
        exists = cur.fetchone() is not None
        conn.close()
        return exists

    def fetch_data(self, pages=10):
        print(f"ğŸ“¡ ë„¤ì´ë²„ ê¸ˆìœµ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {pages}í˜ì´ì§€)...")
        new_count = 0
        
        for page in range(1, pages + 1):
            url = f"https://finance.naver.com/research/invest_list.naver?&page={page}"
            res = requests.get(url)
            soup = BeautifulSoup(res.text, 'html.parser')
            
            table = soup.select_one('table.type_1')
            rows = table.select('tr')
            
            duplicate_in_page = 0
            page_items = 0

            for row in rows:
                cols = row.select('td')
                if len(cols) < 5: continue
                
                page_items += 1
                title = cols[0].text.strip()
                expert_name = cols[1].text.strip()
                date = "20" + cols[4].text.strip().replace('.', '-') # 24.02.02 -> 2024-02-02
                
                # [ì§€ëŠ¥í˜• ì²´í¬] ì´ë¯¸ DBì— ìˆëŠ” ë¦¬í¬íŠ¸ì¸ê°€?
                if self.is_already_exists(title, date, expert_name):
                    duplicate_in_page += 1
                    continue
                
                # ë°ì´í„° ì €ì¥ ë¡œì§ (ì†Œì¥ë‹˜ ê¸°ì¡´ ì½”ë“œì˜ ì €ì¥ ë¶€ë¶„ í˜¸ì¶œ)
                # self.save_to_db(title, expert_name, date, ...) 
                new_count += 1
            
            print(f"ğŸ“„ {page}í˜ì´ì§€ ì™„ë£Œ: {page_items - duplicate_in_page}ê°œ ì‹ ê·œ ìˆ˜ì§‘")
            
            # [í•µì‹¬] í•œ í˜ì´ì§€ì˜ ëª¨ë“  ë¦¬í¬íŠ¸ê°€ ì´ë¯¸ DBì— ìˆë‹¤ë©´, ê³¼ê±° ë°ì´í„°ê°€ ë‹¤ ì±„ì›Œì§„ ê²ƒìœ¼ë¡œ ë³´ê³  ì¤‘ë‹¨
            if page_items > 0 and duplicate_in_page == page_items:
                print(f"ğŸ›‘ {page}í˜ì´ì§€ì—ì„œ ì¤‘ë³µ ë°ì´í„° ë°œê²¬. ìˆ˜ì§‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
                
            time.sleep(0.5) # ì„œë²„ ë¶€í•˜ ë°©ì§€ìš© ë¯¸ì„¸ ì§€ì—°

        print(f"âœ… ì´ {new_count}ê°œì˜ ìƒˆë¡œìš´ ë¦¬í¬íŠ¸ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
